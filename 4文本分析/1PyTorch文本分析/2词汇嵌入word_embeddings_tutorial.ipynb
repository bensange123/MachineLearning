{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Word Embeddings: Encoding Lexical Semantics\n",
    "===========================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f283a0ecef0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Author: Robert Guthrie\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([0]) tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
    "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
    "hello_embed = embeds(lookup_tensor)\n",
    "print(word_to_ix[\"hello\"],lookup_tensor,hello_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 例子: N-Gram 语言模型\n",
    "An Example: N-Gram Language Modeling\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "Recall that in an n-gram language model, given a sequence of words\n",
    "$w$, we want to compute\n",
    "\n",
    "\\begin{align}P(w_i | w_{i-1}, w_{i-2}, \\dots, w_{i-n+1} )\\end{align}\n",
    "\n",
    "Where $w_i$ is the ith word of the sequence.\n",
    "\n",
    "In this example, we will compute the loss function on some training\n",
    "examples and update the parameters with backpropagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['When', 'forty', 'winters'], 'shall'), (['forty', 'winters', 'shall'], 'besiege'), (['winters', 'shall', 'besiege'], 'thy')]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 3\n",
    "EMBEDDING_DIM = 40\n",
    "# 我们将使用 Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,Will be a totter'd weed of small worth held:Then being asked, where\n",
    "all thy beauty lies, Where all the treasure of thy lusty days; To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise. How much more praise deserv'd thy beauty's use,If thou couldst\n",
    "answer 'This fair child of mine Shall sum my count, and make my old excuse,'Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old, And see thy blood warm when thou feel'st it cold. \n",
    "The Continuous Bag-of-Words model (CBOW) is frequently\n",
    "used in NLP deep learning. It is a model that tries to\n",
    "predict words given the context of a few words before \n",
    "and a few words after the target word. This is distinct \n",
    "from language modeling, since CBOW is not sequential and \n",
    "does not have to be probabilistic. Typcially, CBOW is used\n",
    "to quickly train word embeddings, and these embeddings are\n",
    "used to initialize the embeddings of some more complicated\n",
    "model. Usually, this is referred to as pretraining embeddings.\n",
    "It almost always helps performance a couple of percent. We are about to \n",
    "study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.To learn how to use \n",
    "PyTorch, begin with our Getting Started Tutorials. The 60-minute blitz is the \n",
    "most common starting point, and provides a broad view into how to use PyTorch \n",
    "from the basics all the way into constructing deep neural networks.Hello, everyone!\n",
    "My name is and my English name is . I’m 15 years old. I’m a student in a very beautiful\n",
    "school called Liangbing Middle School and I am in Class 1 Grade 7. My hometown Liangbing\n",
    "is also a beautiful town. I’m very happy and I like to make friends with others. \n",
    "I also like singing but traveling is my favorite, I have been to many interesting\n",
    "places in China but I haven’t been to other countries. What a pity! At school, \n",
    "I study Chinese,math, English, history, politics and so on. I like all of them.\n",
    "I often help my teacher take care of my class and I think I am a good helper. \n",
    "I live with my parents and we go home on time every day.When I am at home, I often\n",
    "help my mother do some housework and my mother said I am a good helper, too. My mother\n",
    "is a barber.She cuts hair very well.She is kind.Many people like her. she often teaches\n",
    "me the way of learning well, if you want to learn English well, here is some of my advice.\n",
    "At first , you must read many articles and know many words, if you meet up with some new\n",
    "words, you can look them up in the dictionary, you should know their meanings, how to \n",
    "read them and spell them. If you keep working hard, you will be successful. Then, you\n",
    "ought to speak English as much as possible. Remember an old saying, Nothing is impossible\n",
    "in the world if you put your heart into it. That’s all, thank you. Hello, Everybody!\n",
    "I am very glad to stand here to give you my introduction. I am from Liangbing Middle School.\n",
    "I am studying in Class 1,Grade 7. I love my hometown Liangbing because it’s a very beautiful\n",
    "town. I have many hobbies, such as reading books, listening to music, surfing the Internet\n",
    "and traveling. But listening to music is my favorite. I like pop music best. My idol are \n",
    "Jay. I am a lively girl. I like making friends and chatting with them. I can play the \n",
    "Chinese Kungfu. I passed all ten levels when I was in Grade 5.However, because of the\n",
    "busy study, I don’t have any free time to practise it. What a pity! I am good at Math\n",
    "and English and I like English better. In my opinion, it’s very easy and fun to learn\n",
    "and use English and this contest is a great chance for me to learn English from others.\n",
    "My saying is god help those who help themselves and I will never give up during my course\n",
    "of learning. Now, after listening to my introduction, do you know me well?\"\"\".split()\n",
    "# 我们应该对输入进行标记,但是我们将忽略它\n",
    "# 建造一系列元组.  每个元组 ([ word_i-2, word_i-1 ], 都是目标单词)\n",
    "# trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + CONTEXT_SIZE])\n",
    "#             for i in range(len(test_sentence) - CONTEXT_SIZE)]\n",
    "\n",
    "# 通用的上下文个数\n",
    "trigrams = [([test_sentence[i + i0] for i0 in range(CONTEXT_SIZE)], test_sentence[i + CONTEXT_SIZE])\n",
    "            for i in range(len(test_sentence) - CONTEXT_SIZE)]\n",
    "# 输出前 3, 为了让你看到他的各式\n",
    "print(trigrams[:3])\n",
    "vocab = set(test_sentence)#创建一个无序不重复元素集,删除重复数据\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}#建立单词索引\n",
    "#print(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NGramLanguageModeler(\n",
      "  (embeddings): Embedding(411, 40)\n",
      "  (linear1): Linear(in_features=120, out_features=128, bias=True)\n",
      "  (linear2): Linear(in_features=128, out_features=411, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        '''(vocab_size:97 单词的个数, embedding_dim:10 定义单词的属性维度, context_size:2 上下文语境单词的个数)'''\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(200):\n",
    "    total_loss = torch.Tensor([0])\n",
    "    for context, target in trigrams:\n",
    "\n",
    "        # 步骤 1\\. 准备好进入模型的数据 (例如将单词转换成整数索引,并将其封装在变量中)\n",
    "        context_idxs = [word_to_ix[w] for w in context]# 上下文单词的索引\n",
    "        context_var = autograd.Variable(torch.LongTensor(context_idxs))#封装\n",
    "\n",
    "        # 步骤 2\\. 回调 *积累* 梯度. 在进入一个实例前,需要将之前的实力梯度置零\n",
    "        model.zero_grad()\n",
    "\n",
    "        # 步骤 3\\. 运行反向传播,得到单词的概率分布\n",
    "        log_probs = model(context_var)\n",
    "\n",
    "        # 步骤 4\\. 计算损失函数. (再次注意, Torch需要将目标单词封装在变量中)\n",
    "        loss = loss_function(log_probs, autograd.Variable(\n",
    "            torch.LongTensor([word_to_ix[target]])))\n",
    "\n",
    "        # 步骤 5\\. 反向传播并更新梯度\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.data\n",
    "    losses.append(total_loss.numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAc036CQYVkHbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2829c2a0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.3028\n"
     ]
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()\n",
    "print(losses[-1])  # 在训练集中每次迭代损失都会减小!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203\n",
      "When forty winters shall besiege thy brow,And dig deep trenches in thy beauty's field, Thy youth's proud livery so gazed on now,Will be a totter'd weed of small worth held:Then being asked, where all thy beauty lies, Where all the treasure of thy lusty days; To say, within thine own deep sunken eyes, Were an all-eating shame, and thriftless praise. How much more praise deserv'd thy beauty's use,If thou couldst answer 'This fair child of mine Shall sum my count, and make my old excuse,'Proving his beauty by succession thine! This were to be new made when thou art old, And see thy blood warm when thou feel'st it cold. The Continuous Bag-of-Words model (CBOW) is frequently used in NLP deep learning. It is a model that tries to predict words given the context of a few words after the target word. This is distinct from language modeling, since CBOW is not sequential and does not have to be probabilistic. Typcially, CBOW is used to quickly train word embeddings, and these embeddings are used to initialize the embeddings of some more complicated model. Usually, this is referred to as pretraining embeddings. It almost always helps performance a couple of percent. We are about\n"
     ]
    }
   ],
   "source": [
    "ix_to_word = {ix:w for w,ix in word_to_ix.items()}\n",
    "def prediction(context_test):\n",
    "    context_idxs = [word_to_ix[w] for w in context_test]# 上下文单词的索引\n",
    "    context_test = autograd.Variable(torch.LongTensor(context_idxs))#封装\n",
    "    log_probs = model(context_test)\n",
    "    #print(log_probs)\n",
    "    log_data = log_probs.data.numpy()[0]\n",
    "    Idx = np.where(log_data == np.max(log_data))[0][0]\n",
    "    #print(Idx,np.max(log_data))\n",
    "    target_word = ix_to_word[Idx]\n",
    "    return target_word\n",
    "context_test = ['When','forty','winters']\n",
    "sentence = ['When','forty','winters']\n",
    "context_size = len(context_test)\n",
    "for iter in range(200):\n",
    "    #print(context_test)\n",
    "    word = prediction(context_test)  \n",
    "    sentence.append(word)\n",
    "    context_test[0:context_size-1] = context_test[1:]\n",
    "    context_test[-1] = word\n",
    "    #print(context_test)\n",
    "print(len(sentence))\n",
    "print(\" \".join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203\n",
      "  When\n",
      "  forty\n",
      "  winters\n",
      "  shall\n",
      "  besiege\n",
      "  thy\n",
      "  brow,And\n",
      "  dig\n",
      "  deep\n",
      "  trenches\n",
      "  in\n",
      "  thy\n",
      "  beauty's\n",
      "  field,\n",
      "  Thy\n",
      "  youth's\n",
      "  proud\n",
      "  livery\n",
      "  so\n",
      "  gazed\n",
      "  on\n",
      "  now,Will\n",
      "  be\n",
      "  a\n",
      "  totter'd\n",
      "  weed\n",
      "  of\n",
      "  small\n",
      "  worth\n",
      "  held:Then\n",
      "  being\n",
      "  asked,\n",
      "  where\n",
      "  all\n",
      "  thy\n",
      "  beauty\n",
      "  lies,\n",
      "  Where\n",
      "  all\n",
      "  the\n",
      "  treasure\n",
      "  of\n",
      "  thy\n",
      "  lusty\n",
      "  days;\n",
      "  To\n",
      "  say,\n",
      "  within\n",
      "  thine\n",
      "  own\n",
      "  deep\n",
      "  sunken\n",
      "  eyes,\n",
      "  Were\n",
      "  an\n",
      "  all-eating\n",
      "  shame,\n",
      "  and\n",
      "  thriftless\n",
      "  praise.\n",
      "  How\n",
      "  much\n",
      "  more\n",
      "  praise\n",
      "  deserv'd\n",
      "  thy\n",
      "  beauty's\n",
      "  use,If\n",
      "  thou\n",
      "  couldst\n",
      "  answer\n",
      "  'This\n",
      "  fair\n",
      "  child\n",
      "  of\n",
      "  mine\n",
      "  Shall\n",
      "  sum\n",
      "  my\n",
      "  count,\n",
      "  and\n",
      "  make\n",
      "  my\n",
      "  old\n",
      "  excuse,'Proving\n",
      "  his\n",
      "  beauty\n",
      "  by\n",
      "  succession\n",
      "  thine!\n",
      "  This\n",
      "  were\n",
      "  to\n",
      "  be\n",
      "  new\n",
      "  made\n",
      "  when\n",
      "  thou\n",
      "  art\n",
      "  old,\n",
      "  And\n",
      "  see\n",
      "  thy\n",
      "  blood\n",
      "  warm\n",
      "  when\n",
      "  thou\n",
      "  feel'st\n",
      "  it\n",
      "  cold.\n",
      "  The\n",
      "  Continuous\n",
      "  Bag-of-Words\n",
      "  model\n",
      "  (CBOW)\n",
      "  is\n",
      "  frequently\n",
      "  used\n",
      "  in\n",
      "  NLP\n",
      "  deep\n",
      "  learning.\n",
      "  It\n",
      "  is\n",
      "  a\n",
      "  model\n",
      "  that\n",
      "  tries\n",
      "  to\n",
      "  predict\n",
      "  words\n",
      "  given\n",
      "  the\n",
      "  context\n",
      "  of\n",
      "  a\n",
      "  few\n",
      "  words\n",
      "- before\n",
      "- and\n",
      "- a\n",
      "- few\n",
      "- words\n",
      "  after\n",
      "  the\n",
      "  target\n",
      "  word.\n",
      "  This\n",
      "  is\n",
      "  distinct\n",
      "  from\n",
      "  language\n",
      "  modeling,\n",
      "  since\n",
      "  CBOW\n",
      "  is\n",
      "  not\n",
      "  sequential\n",
      "  and\n",
      "  does\n",
      "  not\n",
      "  have\n",
      "  to\n",
      "  be\n",
      "  probabilistic.\n",
      "  Typcially,\n",
      "  CBOW\n",
      "  is\n",
      "  used\n",
      "  to\n",
      "  quickly\n",
      "  train\n",
      "  word\n",
      "  embeddings,\n",
      "  and\n",
      "  these\n",
      "  embeddings\n",
      "  are\n",
      "  used\n",
      "  to\n",
      "  initialize\n",
      "  the\n",
      "  embeddings\n",
      "  of\n",
      "  some\n",
      "  more\n",
      "  complicated\n",
      "  model.\n",
      "  Usually,\n",
      "  this\n",
      "  is\n",
      "  referred\n",
      "  to\n",
      "  as\n",
      "  pretraining\n",
      "  embeddings.\n",
      "  It\n",
      "  almost\n",
      "  always\n",
      "  helps\n",
      "  performance\n",
      "  a\n",
      "  couple\n",
      "+ of\n",
      "+ percent.\n",
      "+ We\n",
      "+ are\n",
      "+ about\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "\n",
    "text1_lines = (\" \".join(test_sentence[0:203])).split()\n",
    "print(len(text1_lines))\n",
    "text2_lines = (\" \".join(sentence)).split()\n",
    "d = difflib.Differ()\n",
    "diff = d.compare(text1_lines, text2_lines)\n",
    "\n",
    "print(\"\\n\".join(list(diff)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dig', 'deep', 'trenches']\n",
      "['dig', 'deep', 'trenches']\n"
     ]
    }
   ],
   "source": [
    "context_test = ['And', 'dig','deep','trenches']\n",
    "a = [word1 for word1 in context_test[1:]]\n",
    "print(a)\n",
    "print(context_test[1:])\n",
    "c = [-10,-5,0,5,3,10,15,-20,25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算单词嵌入: 连续单词包\n",
    " Exercise: Computing Word Embeddings: Continuous Bag-of-Words\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "The Continuous Bag-of-Words model (CBOW) is frequently used in NLP deep\n",
    "learning. It is a model that tries to predict words given the context of\n",
    "a few words before and a few words after the target word. This is\n",
    "distinct from language modeling, since CBOW is not sequential and does\n",
    "not have to be probabilistic. Typcially, CBOW is used to quickly train\n",
    "word embeddings, and these embeddings are used to initialize the\n",
    "embeddings of some more complicated model. Usually, this is referred to\n",
    "as *pretraining embeddings*. It almost always helps performance a couple\n",
    "of percent.\n",
    "\n",
    "The CBOW model is as follows. Given a target word $w_i$ and an\n",
    "$N$ context window on each side, $w_{i-1}, \\dots, w_{i-N}$\n",
    "and $w_{i+1}, \\dots, w_{i+N}$, referring to all context words\n",
    "collectively as $C$, CBOW tries to minimize\n",
    "\n",
    "\\begin{align}-\\log p(w_i | C) = -\\log \\text{Softmax}(A(\\sum_{w \\in C} q_w) + b)\\end{align}\n",
    "\n",
    "where $q_w$ is the embedding for word $w$.\n",
    "\n",
    "Implement this model in Pytorch by filling in the class below. Some\n",
    "tips:\n",
    "\n",
    "* Think about which parameters you need to define.\n",
    "* Make sure you know what shape each operation expects. Use .view() if you need to\n",
    "  reshape.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['We', 'are', 'to', 'study'], 'about'), (['are', 'about', 'study', 'the'], 'to'), (['about', 'to', 'the', 'idea'], 'study'), (['to', 'study', 'idea', 'of'], 'the'), (['study', 'the', 'of', 'a'], 'idea')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([32, 14, 21,  8])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "# By deriving a set from `raw_text`, we deduplicate the array\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))\n",
    "print(data[:5])\n",
    "\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        pass\n",
    "\n",
    "# create your model and train.  here are some functions to help you make\n",
    "# the data ready for use by your module\n",
    "\n",
    "\n",
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "make_context_vector(data[0][0], word_to_ix)  # example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
