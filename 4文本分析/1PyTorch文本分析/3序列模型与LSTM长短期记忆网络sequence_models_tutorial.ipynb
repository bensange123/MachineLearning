{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_style": "center",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Sequence Models and Long-Short Term Memory Networks\n",
    "===================================================\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5e70539550>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Author: Robert Guthrie\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      " [tensor([[-0.5525,  0.6355, -0.3968]]), tensor([[-0.6571, -1.6428,  0.9803]]), tensor([[-0.0421, -0.8206,  0.3133]]), tensor([[-1.1352,  0.3773, -0.2824]]), tensor([[-2.5667, -1.4303,  0.5009]])]\n",
      "hidden:\n",
      " (tensor([[[ 0.5438, -0.4057,  1.1341]]]), tensor([[[-1.1115,  0.3501, -0.7703]]]))\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(3, 3)  # 输入维度是3, 输出维度也是3\n",
    "inputs = [autograd.Variable(torch.randn((1, 3)))\n",
    "          for _ in range(5)]  # 构造一个长度为5的序列\n",
    "print('inputs:\\n',inputs)\n",
    "# 初始化隐藏状态\n",
    "# 实际上是初始化了隐藏元和记忆元（其实就是用来存储当前lstm单元的输出的）\n",
    "# 初始化的隐藏元和记忆元,通常它们是维度是一样的\n",
    "hidden = (autograd.Variable(torch.randn(1, 1, 3)),\n",
    "          autograd.Variable(torch.randn(1, 1, 3)))# 1个LSTM层，1个batch,隐藏元维度3\n",
    "print('hidden:\\n',hidden)\n",
    "for i in inputs:\n",
    "    # 将序列的元素逐个输入到LSTM\n",
    "    # 经过每步操作,hidden 的值包含了隐藏状态的信息\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "    # print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用于解释LSTM的输入：$$初始化的隐藏单元C_{t-1}+记忆单元S_{t-1}+当前序列X_t$$\n",
    "![Image of Yaktocat](https://img-blog.csdn.net/20180518110325328?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lveW9mdTAwNw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5525,  0.6355, -0.3968],\n",
      "        [-0.6571, -1.6428,  0.9803],\n",
      "        [-0.0421, -0.8206,  0.3133],\n",
      "        [-1.1352,  0.3773, -0.2824],\n",
      "        [-2.5667, -1.4303,  0.5009]]) 5\n"
     ]
    }
   ],
   "source": [
    "print(torch.cat(inputs),len(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out:\n",
      " tensor([[[-0.0187,  0.1713, -0.2944]],\n",
      "\n",
      "        [[-0.3521,  0.1026, -0.2971]],\n",
      "\n",
      "        [[-0.3191,  0.0781, -0.1957]],\n",
      "\n",
      "        [[-0.1634,  0.0941, -0.1637]],\n",
      "\n",
      "        [[-0.3368,  0.0959, -0.0538]]], grad_fn=<CatBackward>)\n",
      "hidden:\n",
      " (tensor([[[-0.3368,  0.0959, -0.0538]]], grad_fn=<ViewBackward>), tensor([[[-0.9825,  0.4715, -0.0633]]], grad_fn=<ViewBackward>))\n"
     ]
    }
   ],
   "source": [
    "# 另外, 我们还可以一次对整个序列进行训练. LSTM 返回的第一个值表示所有时刻的隐状态值,\n",
    "# 第二个值表示最近的隐状态值 (因此下面的 \"out\"的最后一个值和 \"hidden\" 的值是一样的).\n",
    "# 之所以这样设计, 是为了通过 \"out\" 的值来获取所有的隐状态值, 而用 \"hidden\" 的值来\n",
    "# 进行序列的反向传播运算, 具体方式就是将它作为参数传入后面的 LSTM 网络.\n",
    "\n",
    "# 增加额外的第二个维度\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "# 清空输出隐状态\n",
    "hidden = (autograd.Variable(torch.randn(1, 1, 3)), autograd.Variable(torch.randn((1, 1, 3))))  \n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print('out:\\n',out)\n",
    "print('hidden:\\n',hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(len(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# from tensorboardX import SummaryWriter\n",
    "# dummy_input = inputs[0].view(1,1,-1)[0]\n",
    "# print(dummy_input)\n",
    "# with SummaryWriter(comment='VGG_self') as w:\n",
    "#     w.add_graph(lstm, (dummy_input,hidden,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用 LSTM 来进行词性标注 Example: An LSTM for Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'that': 7, 'dog': 1, 'book': 8, 'Everybody': 5, 'ate': 2, 'The': 0, 'read': 6, 'apple': 4, 'the': 3}\n"
     ]
    }
   ],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "\n",
    "# 生成单词向索引的词典\n",
    "# word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "word_to_ix = {}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "\n",
    "# 实际中通常使用更大的维度如32维, 64维.\n",
    "# 这里我们使用小的维度, 为了方便查看训练过程中权重的变化.\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 3\n"
     ]
    }
   ],
   "source": [
    "print(len(word_to_ix),len(tag_to_ix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    # model = LSTMTagger(embedding_dim = 6, hidden_dim = 6, vocab_size = 9, tagset_size = 3)\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # 为单词编码\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        #  LSTM 以 word_embeddings 作为输入, 输出维度为 hidden_dim 的隐状态值\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim) # 输入维度=embedding_dim,输出维度=hidden_dim\n",
    "\n",
    "        # 线性层将隐状态空间映射到标注空间\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # 开始时刻, 没有隐状态\n",
    "        # 关于维度设置的详情,请参考 Pytorch 文档\n",
    "        # 各个维度的含义是 (num_layers, minibatch_size, hidden_dim)\n",
    "        return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)),\n",
    "                autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        #将整个句子进行编码\n",
    "        embeds = self.word_embeddings(sentence)#sentence：句子对应的下标索引\n",
    "        \n",
    "        lstm_out, self.hidden = self.lstm(embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        \n",
    "        # print(lstm_out.size(),'\\n',lstm_out.view(len(sentence), -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        # print(tag_space)\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMTagger(\n",
      "  (word_embeddings): Embedding(9, 6)\n",
      "  (lstm): LSTM(6, 6)\n",
      "  (hidden2tag): Linear(in_features=6, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['The', 'dog', 'ate', 'the', 'apple'], ['DET', 'NN', 'V', 'DET', 'NN']), (['Everybody', 'read', 'that', 'book'], ['NN', 'V', 'DET', 'NN'])]\n",
      "tensor([0, 1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# nn.LSTM?\n",
    "print(training_data)\n",
    "inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1389, -1.2024, -0.9693],\n",
      "        [-1.1065, -1.2200, -0.9834],\n",
      "        [-1.1286, -1.2093, -0.9726],\n",
      "        [-1.1190, -1.1960, -0.9916],\n",
      "        [-1.0137, -1.2642, -1.0366]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 查看下训练前得分的值\n",
    "# 注意: 输出的 i,j 元素的值表示单词 i 的 j 标签的得分\n",
    "inputs = prepare_sequence(training_data[0][0], word_to_ix)#输出句子中单词对应的下标索引tensor([0, 1, 2, 3, 4])\n",
    "tag_scores = model(inputs)\n",
    "print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0858, -2.9355, -3.5374],\n",
      "        [-5.2313, -0.0234, -4.0314],\n",
      "        [-3.9098, -4.1279, -0.0368],\n",
      "        [-0.0187, -4.7809, -4.5960],\n",
      "        [-5.8170, -0.0183, -4.1879]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(300):  # 再次说明下, 实际情况下你不会训练300个周期, 此例中我们只是构造了一些假数据\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1\\. 请记住 Pytorch 会累加梯度\n",
    "        # 每次训练前需要清空梯度值\n",
    "        model.zero_grad()\n",
    "\n",
    "        # 此外还需要清空 LSTM 的隐状态\n",
    "        # 将其从上个实例的历史中分离出来\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        # Step 2\\. 准备网络输入, 将其变为词索引的 Variables 类型数据\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3\\. 前向传播\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4\\. 计算损失和梯度值, 通过调用 optimizer.step() 来更新梯度\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# 查看训练后得分的值\n",
    "inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "tag_scores = model(inputs)\n",
    "# 句子是 \"the dog ate the apple\", i,j 表示对于单词 i, 标签 j 的得分.\n",
    "# 我们采用得分最高的标签作为预测的标签. 从下面的输出我们可以看到, 预测得\n",
    "# 到的结果是0 1 2 0 1\\. 因为 索引是从0开始的, 因此第一个值0表示第一行的\n",
    "# 最大值, 第二个值1表示第二行的最大值, 以此类推. 所以最后的结果是 DET\n",
    "# NOUN VERB DET NOUN, 整个序列都是正确的!\n",
    "print(tag_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Augmenting the LSTM part-of-speech tagger with character-level features\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "In the example above, each word had an embedding, which served as the\n",
    "inputs to our sequence model. Let's augment the word embeddings with a\n",
    "representation derived from the characters of the word. We expect that\n",
    "this should help significantly, since character-level information like\n",
    "affixes have a large bearing on part-of-speech. For example, words with\n",
    "the affix *-ly* are almost always tagged as adverbs in English.\n",
    "\n",
    "To do this, let $c_w$ be the character-level representation of\n",
    "word $w$. Let $x_w$ be the word embedding as before. Then\n",
    "the input to our sequence model is the concatenation of $x_w$ and\n",
    "$c_w$. So if $x_w$ has dimension 5, and $c_w$\n",
    "dimension 3, then our LSTM should accept an input of dimension 8.\n",
    "\n",
    "To get the character level representation, do an LSTM over the\n",
    "characters of a word, and let $c_w$ be the final hidden state of\n",
    "this LSTM. Hints:\n",
    "\n",
    "* There are going to be two LSTM's in your new model.\n",
    "  The original one that outputs POS tag scores, and the new one that\n",
    "  outputs a character-level representation of each word.\n",
    "* To do a sequence model over characters, you will have to embed characters.\n",
    "  The character embeddings will be the input to the character LSTM.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:deeplearning]",
   "language": "python",
   "name": "conda-env-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
